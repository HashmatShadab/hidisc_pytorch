infra:
  log_dir: ./ # where all the experiments are
  exp_name: hidisc # create a subdirectory for each set of experiments
  comment: patient_disc_dev # can use this to customise for each experiment
  seed: 1000

root: "."
out_dir: "OUTDIR"
eval_only:


wandb:
  project: "HiDisc"
  entity: "hashmatshadab"
  mode: "online"
  use:
  exp_name: "main"


data:
  db_root: F:\Code\datasets\hidisc_data_small
  meta_json: "opensrh.json"
  meta_split_json: "train_val_split.json"
  dynamic_aug: False
  dynamic_aug_version: "v0"
  train_augmentation:
    - which: random_horiz_flip
      params: {}
    - which: random_vert_flip
      params: {}
    - which: gaussian_noise
      params: {}
    - which: color_jitter
      params: {}
    - which: random_autocontrast
      params: {}
    - which: random_solarize
      params:
        threshold: 0.2
    - which: random_sharpness
      params:
        sharpness_factor: 2
    - which: gaussian_blur
      params:
        kernel_size: 5
        sigma: 1
    - which: random_affine
      params:
        degrees: 10
        translate: [0.1, 0.3]
    - which: random_resized_crop
      params:
        size: 300
    - which: random_erasing
      params: {}
  valid_augmentation:
    - which: random_horiz_flip
      params: {}
    - which: random_vert_flip
      params: {}
    - which: gaussian_noise
      params: {}
    - which: color_jitter
      params: {}
    - which: random_autocontrast
      params: {}
    - which: random_solarize
      params:
        threshold: 0.2
    - which: random_sharpness
      params:
        sharpness_factor: 2
    - which: gaussian_blur
      params:
        kernel_size: 5
        sigma: 1
    - which: random_affine
      params:
        degrees: 10
        translate: [0.1, 0.3]
    - which: random_resized_crop
      params:
        size: 300
    - which: random_erasing
      params: {}
  rand_aug_prob: 0.3
  hidisc:
    num_slide_samples: 2
    num_patch_samples: 2
    num_transforms: 2
  balance_study_per_class: true
model:
  backbone: resnet50
  mlp_hidden: []
  num_embedding_out: 128
  restart_from_ckpt:
  checkpoints_path: ""
  freeze_level: -1



training:
  dynamic_weights_lamda: 0.66
  only_adv: False
  objective:
    which: hidisc
    params:
      lambda_patient: 1.0
      lambda_slide: 1.0
      lambda_patch: 1.0
      supcon_params:
        temperature: 0.07
        base_temperature: 0.07
        contrast_mode: all
  batch_size: 10
  num_epochs: 40000
  optimizer: adamw # [sgd, adam, adamw]
  learn_rate: 1.0e-3
  attack:
      name: none
      eps: 8.0
      alpha: 2.0
      iters: 5
      loss_type: "p_s_pt"
      warmup_epochs: 0
  scheduler:
    which: cos_warmup
    params:
      num_warmup_steps: 0.1
      num_cycles: 0.5
  imagenet_backbone_checkpoint: null
  save_checkpoint_interval: 1000

distributed:
  single_gpu: False
  world_size: 1
  local_rank: -1
  dist_on_itp: False
  dist_url: "env://"
  find_unused_params:
  rank:
  gpu:
  distributed:
  dist_backend:

hydra:
  run:
    dir: ${root}/${out_dir}
  job:
    chdir: True
#    config:
#      override_dirname:
#        kv_sep: "_"
#        item_sep: "_"
#        exclude_keys:
#          - out_dir
#          - root
#          - eval_only
#          - model.restart_from_ckpt
#          - model.name
#          - train.epochs

